{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1100d48-290a-43ba-8b18-f14520082d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoModel(\n",
       "  (wte): Embedding(50259, 768)\n",
       "  (wpe): Embedding(2048, 768)\n",
       "  (drop): Dropout(p=0, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Get our models - The package will take care of downloading the models automatically\n",
    "# For best performance: Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\")\n",
    "model = AutoModel.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\")\n",
    "# Deactivate Dropout (There is no dropout in the above models so it makes no difference here but other SGPT models may have dropout)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23df600c-415e-452e-bc9b-ba75506a1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"I'm searching for a planet not too far from Earth.\",\n",
    "]\n",
    "\n",
    "docs = [\n",
    "    \"Neptune is the eighth and farthest-known Solar planet from the Sun. In the Solar System, it is the fourth-largest planet by diameter, the third-most-massive planet, and the densest giant planet. It is 17 times the mass of Earth, slightly more massive than its near-twin Uranus.\",\n",
    "    \"Mars is the fourth planet from the Sun and the second-smallest planet in the Solar System, only being larger than Mercury. In the English language, Mars is named for the Roman god of war. Mars is a terrestrial planet with a thin atmosphere (less than 1% that of Earth's), and has a crust primarily composed of elements similar to Earth's crust, as well as a core made of iron and nickel. Mars has surface features such as impact craters, valleys, dunes and polar ice caps. It has two small and irregularly shaped moons, Phobos and Deimos.\",\n",
    "    \"TRAPPIST-1d, also designated as 2MASS J23062928-0502285 d, is a small exoplanet (about 30% the mass of the earth), which orbits on the inner edge of the habitable zone of the ultracool dwarf star TRAPPIST-1 approximately 40 light-years (12.1 parsecs, or nearly 3.7336×1014 km) away from Earth in the constellation of Aquarius.\",\n",
    "    \"A harsh desert world orbiting twin suns in the galaxy’s Outer Rim, Tatooine is a lawless place ruled by Hutt gangsters. Many settlers scratch out a living on moisture farms, while spaceport cities such as Mos Eisley and Mos Espa serve as home base for smugglers, criminals, and other rogues.\",\n",
    "]\n",
    "\n",
    "SPECB_QUE_BOS = tokenizer.encode(\"[\", add_special_tokens=False)[0]\n",
    "SPECB_QUE_EOS = tokenizer.encode(\"]\", add_special_tokens=False)[0]\n",
    "\n",
    "SPECB_DOC_BOS = tokenizer.encode(\"{\", add_special_tokens=False)[0]\n",
    "SPECB_DOC_EOS = tokenizer.encode(\"}\", add_special_tokens=False)[0]\n",
    "\n",
    "\n",
    "def tokenize_with_specb(texts, is_query):\n",
    "    # Tokenize without padding\n",
    "    batch_tokens = tokenizer(texts, padding=False, truncation=True)   \n",
    "    # Add special brackets & pay attention to them\n",
    "    for seq, att in zip(batch_tokens[\"input_ids\"], batch_tokens[\"attention_mask\"]):\n",
    "        if is_query:\n",
    "            seq.insert(0, SPECB_QUE_BOS)\n",
    "            seq.append(SPECB_QUE_EOS)\n",
    "        else:\n",
    "            seq.insert(0, SPECB_DOC_BOS)\n",
    "            seq.append(SPECB_DOC_EOS)\n",
    "        att.insert(0, 1)\n",
    "        att.append(1)\n",
    "    # Add padding\n",
    "    batch_tokens = tokenizer.pad(batch_tokens, padding=True, return_tensors=\"pt\")\n",
    "    return batch_tokens\n",
    "\n",
    "\n",
    "def get_weightedmean_embedding(batch_tokens, last_hidden_state):\n",
    "    # Get the embeddings\n",
    " \n",
    "    # Get weights of shape [bs, seq_len, hid_dim]\n",
    "    weights = (\n",
    "        torch.arange(start=1, end=last_hidden_state.shape[1] + 1)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float().to(last_hidden_state.device)\n",
    "    )\n",
    "\n",
    "    # Get attn mask of shape [bs, seq_len, hid_dim]\n",
    "    input_mask_expanded = (\n",
    "        batch_tokens[\"attention_mask\"]\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "    # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)\n",
    "    sum_mask = torch.sum(input_mask_expanded * weights, dim=1)\n",
    "\n",
    "    embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding(batch_tokens, model):\n",
    "    with torch.no_grad():\n",
    "        # Get hidden state of shape [bs, seq_len, hid_dim]\n",
    "        last_hidden_state = model(**batch_tokens, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "    return get_weightedmean_embedding(batch_tokens, last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b45fc3-729d-4730-b1c2-bca1020e7e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Cosine similarity between \"I'm searching for a planet not too far from Earth.\" and \"Neptune is the eight...\" is: 0.622\n",
      "Cosine similarity between \"I'm searching for a planet not too far from Earth.\" and \"Mars is the fourth p...\" is: 0.570\n",
      "Cosine similarity between \"I'm searching for a planet not too far from Earth.\" and \"TRAPPIST-1d, also de...\" is: 0.490\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = get_embedding(tokenize_with_specb(queries, is_query=True), model)\n",
    "doc_embeddings = get_embedding(tokenize_with_specb(docs, is_query=False), model)\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(query_embeddings[0], doc_embeddings[0])\n",
    "cosine_sim_0_2 = 1 - cosine(query_embeddings[0], doc_embeddings[1])\n",
    "cosine_sim_0_3 = 1 - cosine(query_embeddings[0], doc_embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (queries[0], docs[0][:20] + \"...\", cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (queries[0], docs[1][:20] + \"...\", cosine_sim_0_2))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (queries[0], docs[2][:20] + \"...\", cosine_sim_0_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1159aa9b-0fcc-4155-ba7c-4d10df36b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"artifacts/sbert_base\")\n",
    "model.save_pretrained(\"artifacts/sbert_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825dad63-e653-4ac0-970f-82eefc812fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Terminal command!!\n",
    "to create the ONNX model, note there is some floating point error introduced in the ONNX model, I had to reduce the requirements to get the model to pass this health check  (atol param), defaults to 5e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf96d3d-7f52-40e0-a903-fe8cda46babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Using framework PyTorch: 1.12.1\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:557: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:195: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/onnx/__main__.py\", line 107, in <module>\n",
      "    main()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/onnx/__main__.py\", line 100, in main\n",
      "    validate_model_outputs(onnx_config, preprocessor, model, args.output, onnx_outputs, args.atol)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/onnx/convert.py\", line 350, in validate_model_outputs\n",
      "    from onnxruntime import InferenceSession, SessionOptions\n",
      "ModuleNotFoundError: No module named 'onnxruntime'\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --model=artifacts/sbert_base --atol=5e-3 artifacts/onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1118073e-d751-4266-aa91-7e22aef9412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4057a352-8c46-4def-8f83-c6d825737e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession('artifacts/onnx/model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9c9f4cb-1a98-43ae-8503-d45e98bd67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = sess.get_inputs()[0].name\n",
    "embedding_layer_name = sess.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec30adc4-5aee-4d60-94eb-a9b454493bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_onnx(texts, is_query=True):\n",
    "    model_inputs = tokenize_with_specb(texts, is_query=is_query)\n",
    "    inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}\n",
    "\n",
    "    # List of embeddings\n",
    "    sequence = torch.from_numpy(sess.run(None, inputs_onnx)[0])\n",
    "\n",
    "    # Weighted sum, using the attention weights\n",
    "    embeddings = get_weightedmean_embedding(model_inputs, sequence)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcc68a76-6cae-409d-8e33-09ea0e05d2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n"
     ]
    }
   ],
   "source": [
    "qembs = embed_onnx(queries, is_query=True)\n",
    "docembs = embed_onnx(docs, is_query=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21946c52-ea33-4b96-8e1b-1d1244956bf3",
   "metadata": {},
   "source": [
    "# Notice that the resuls are identical to the huggingface model above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8570de62-2747-4a68-9905-5e353062a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"I'm searching for a planet not too far from Earth.\" and \"Neptune is the eight...\" is: 0.622\n",
      "Cosine similarity between \"I'm searching for a planet not too far from Earth.\" and \"Mars is the fourth p...\" is: 0.570\n",
      "Cosine similarity between \"I'm searching for a planet not too far from Earth.\" and \"TRAPPIST-1d, also de...\" is: 0.490\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_0_1 = 1 - cosine(qembs[0], docembs[0])\n",
    "cosine_sim_0_2 = 1 - cosine(qembs[0], docembs[1])\n",
    "cosine_sim_0_3 = 1 - cosine(qembs[0], docembs[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (queries[0], docs[0][:20] + \"...\", cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (queries[0], docs[1][:20] + \"...\", cosine_sim_0_2))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (queries[0], docs[2][:20] + \"...\", cosine_sim_0_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28ab1b85-a0b1-49f3-a44a-2879dbc5de00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = np.average(sequence, axis=1)\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a6087-9f78-43c4-9d54-ad7be8d735b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGeo",
   "language": "python",
   "name": "pytorchgeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
